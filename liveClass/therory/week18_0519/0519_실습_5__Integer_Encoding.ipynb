{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Integer Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 정수 인코딩 챕터 튜토리얼 자료입니다.  \n",
        "\n",
        "링크 : https://wikidocs.net/31766"
      ],
      "metadata": {
        "id": "KKidbdKZMOGv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTh-I1nypjMG"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 정수 인코딩(Integer Encoding)"
      ],
      "metadata": {
        "id": "8-SZNeaMLytr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) dictionary 사용하기"
      ],
      "metadata": {
        "id": "NZolhMgOL1RG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41XK9iPpdjw"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brBb-QZVpfSH"
      },
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxZ2dSPUpg62"
      },
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfA5es2Xpht2"
      },
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result) \n",
        "print(preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L5i-fJ1p-jv"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6sBmoKnqJNM"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThZt6GvMqKWG"
      },
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNstQGVRqLqG"
      },
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
        "        i = i + 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVAiOfO1qPJP"
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2E0n6WEqau3"
      },
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRUpQCN5q1i4"
      },
      "source": [
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brBkMS4DqdOP"
      },
      "source": [
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Counter 사용하기"
      ],
      "metadata": {
        "id": "jbMTe3vcL4a6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZkI_y03qsfY"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlVjBnQsq6r-"
      },
      "source": [
        "print(preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICOgMShDq9T-"
      },
      "source": [
        "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
        "all_words_list = sum(preprocessed_sentences, [])\n",
        "print(all_words_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr-RD6mdrNW9"
      },
      "source": [
        "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
        "vocab = Counter(all_words_list)\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXUzdzJrRe4"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Rk9wBurSxo"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4_HjiI8rTmf"
      },
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab :\n",
        "    i = i + 1\n",
        "    word_to_index[word] = i\n",
        "    \n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) NLTK의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "VLHS225wL-F5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-ObWwGYrfUv"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oui6gPOprhYP"
      },
      "source": [
        "vocab = FreqDist(np.hstack(preprocessed_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOSMtWCQrlYf"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZrR9971rmtX"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268TWlpZrqOH"
      },
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) enumerate 이해하기"
      ],
      "metadata": {
        "id": "TnfqyhEhMHif"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAIXuuDirtuv"
      },
      "source": [
        "test_input = ['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
        "  print(\"value : {}, index: {}\".format(value, index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 케라스(Keras)의 텍스트 전처리"
      ],
      "metadata": {
        "id": "1pS_DRLtMJl7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvjYhEkdryBf"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNyMMfEBrz1n"
      },
      "source": [
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVxlasf6r7p9"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
        "tokenizer.fit_on_texts(preprocessed_sentences) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WySIFxB3r-JP"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjn6u4qr_HP"
      },
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF-ar0-Xr_9v"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renGtU4fsBwn"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TXcbEPSsD-v"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po1w9WpNsFD_"
      },
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0sbhhBKsGSH"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IALRN6TDsJ4f"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17948a0HsQaH"
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for word in words_frequency:\n",
        "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrfGD7f6stSf"
      },
      "source": [
        "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrrRAUozsymH"
      },
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qfcu_Y8s0_f"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}