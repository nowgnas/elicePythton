{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng/kor_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 영어/한국어 Word2Vec 실습 학습 자료입니다.  \n",
        "링크 : https://wikidocs.net/50739"
      ],
      "metadata": {
        "id": "IzrLHh2YP1Ge"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sdE4eTfLW8b"
      },
      "source": [
        "import gensim\n",
        "gensim.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58G-gTlKL_Na"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvthgGw5LHNW"
      },
      "source": [
        "# 1. 영어 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVrVWEcLCIG"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60AESIY1LFrN"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "id": "NCc2n8IUi06u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZI00TbvLKCI"
      },
      "source": [
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUqiRKAmLLg9"
      },
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA1Kr9UkLMfL"
      },
      "source": [
        "# 샘플 3개만 출력\n",
        "for line in result[:3]:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3gtDRcnLw3V"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dx0cWsTLRsI"
      },
      "source": [
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWD9seUhLl2_"
      },
      "source": [
        "여기서 Word2Vec의 인자는 다음과 같습니다.  \n",
        "\n",
        "* size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
        "* window = 컨텍스트 윈도우 크기\n",
        "* min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
        "* workers = 학습을 위한 프로세스 수\n",
        "* sg = 0은 CBOW, 1은 Skip-gram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVmN8NWjLkdg"
      },
      "source": [
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMWDIDJLsL7"
      },
      "source": [
        "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFXguci-L2Gz"
      },
      "source": [
        "model_result = loaded_model.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGHAvgHcL6rC"
      },
      "source": [
        "# 2. 한국어 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaMole3_L8bI"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfIOkE-2L9OA"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7tZNSAtMI0e"
      },
      "source": [
        "train_data = pd.read_table('ratings.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1vT3b5-MJAX"
      },
      "source": [
        "train_data[:5] # 상위 5개 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgoCGCRZMJu_"
      },
      "source": [
        "print('리뷰 개수 :',len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu8cR1VRMMwP"
      },
      "source": [
        "print('NULL 값 존재 유무 :', train_data.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGan-hOTMNw-"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print('NULL 값 존재 유무 :', train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5LwmDrIMY8g"
      },
      "source": [
        "print('리뷰 개수 :',len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KVYwgJLMbgy"
      },
      "source": [
        "# 정규 표현식을 통한 한글 외 문자 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFdpdvDSMdEw"
      },
      "source": [
        "train_data[:5] # 상위 5개 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tpwym-gMg1u"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpOiqZuFMkie"
      },
      "source": [
        "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_data = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    tokenized_data.append(stopwords_removed_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inQezjEYMsMC"
      },
      "source": [
        "print(tokenized_data[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NWAUGLRMtBa"
      },
      "source": [
        "# 리뷰 길이 분포 확인\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
        "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJSFpoy1M4Cu"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJ2VO_VM3G6"
      },
      "source": [
        "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHomXzdzM4lj"
      },
      "source": [
        "print('완성된 임베딩 매트릭스의 크기 확인 :', model.wv.vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ0pNuEjM55g"
      },
      "source": [
        "print(model.wv.most_similar(\"최민식\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu54eXfcM95n"
      },
      "source": [
        "print(model.wv.most_similar(\"히어로\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q9ZZ2RrP7HQ"
      },
      "source": [
        "print(model.wv.most_similar(\"발연기\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 사전 훈련된 Word2Vec"
      ],
      "metadata": {
        "id": "G285zFXbP3cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "Mo7CiIf7Qgfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "id": "uCA1X9owP6pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model.vectors.shape)"
      ],
      "metadata": {
        "id": "VhYcivz5QLcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model.similarity('this', 'is'))\n",
        "print(word2vec_model.similarity('post', 'book'))"
      ],
      "metadata": {
        "id": "qqfBJVoNQLAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model['book'])"
      ],
      "metadata": {
        "id": "81xg-xSDQOx0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}